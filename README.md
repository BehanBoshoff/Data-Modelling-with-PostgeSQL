The purpose of this database is to provide a convenient and efficient source of information for Sparkify to read various data metrics from.
These data metrics can be aggregated values, ad-hoc (unpredictable), and must be returned in a timely manner.
Therefore reading straight from the production DB will not work, thus this database is necessary.
    
My schema design was generated by analysing the log data provided, normalizing that data into 3NF, and then denormilizing the 3NF schema to 
1NF or 2NF for specific queries.
This normalization and demormalization process was done firstly so that ad-hoc queries can easily be written from the 3NF tables, and 
secondly so that established queries
can be run on tables that negate the need for large joins (and so are optimised for read-time).
    
Python scripts can be run in a terminal or launcher using the keyword "run" followed by the script name.
    
File Descriptions:
    
    data: contains dataset used in project.
    create_tables.py: script used for creating the DB, dropping tables and creating tables.
    sql_queries.py: a script containing strings that represent all SQL queries used in the project.
    etl.ipynb: Jupyter notebook used to test each processing step at a modular level, inserting a single record into tables.
    etl.py: a script used to perform ETL processing on the whole dataset.
    test.ipynb: Jupyter notebook used to test whether the data has been loaded correctly.
